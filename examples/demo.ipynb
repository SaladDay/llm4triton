{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# llm4triton 项目演示\n",
        "\n",
        "这个notebook展示了llm4triton项目的核心功能：一个标准化的评估框架，用于测试和优化PyTorch模型到Triton的性能。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 环境设置\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "项目根目录: /home/fanjingluo/explore/llm4triton\n",
            "当前工作目录: /home/fanjingluo/explore/llm4triton\n",
            "CUDA可用: True\n",
            "GPU设备: NVIDIA A800 80GB PCIe\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# 获取项目根目录\n",
        "ROOT = Path.cwd().parent\n",
        "\n",
        "# 切换到项目根目录\n",
        "os.chdir(ROOT)\n",
        "\n",
        "# 添加项目根目录到路径\n",
        "if str(ROOT) not in sys.path:\n",
        "    sys.path.append(str(ROOT))\n",
        "\n",
        "from harness.cases import make_case_forward, load_case_manifest\n",
        "from harness.runner import execute_variant\n",
        "from harness.data import seed_rng\n",
        "\n",
        "print(f\"项目根目录: {ROOT}\")\n",
        "print(f\"当前工作目录: {Path.cwd()}\")\n",
        "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU设备: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 加载测试案例\n",
        "\n",
        "项目使用manifest.yaml来定义测试案例，让我们加载example_linear案例：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "案例名称: example_linear\n",
            "描述: Two-layer MLP forward pass with GELU activation.\n",
            "模型类: FeedForward\n",
            "目标设备: cuda\n",
            "\n",
            "输入配置:\n",
            "{\n",
            "  \"args\": [\n",
            "    {\n",
            "      \"name\": \"x\",\n",
            "      \"shape\": [\n",
            "        16,\n",
            "        1024\n",
            "      ],\n",
            "      \"dtype\": \"float32\",\n",
            "      \"distribution\": \"normal\",\n",
            "      \"mean\": 0.0,\n",
            "      \"std\": 1.0\n",
            "    }\n",
            "  ],\n",
            "  \"kwargs\": {}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# 加载案例配置\n",
        "manifest = load_case_manifest(\"example_linear\")\n",
        "\n",
        "print(f\"案例名称: {manifest.display_name}\")\n",
        "print(f\"描述: {manifest.description}\")\n",
        "print(f\"模型类: {manifest.model_class}\")\n",
        "print(f\"目标设备: {manifest.device}\")\n",
        "print(f\"\\n输入配置:\")\n",
        "print(json.dumps(manifest.inputs, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 查看模型结构\n",
        "\n",
        "让我们实例化模型并查看其结构：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForward(\n",
            "  (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "  (act): GELU(approximate='none')\n",
            "  (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            ")\n",
            "\n",
            "参数数量: 8,393,728\n"
          ]
        }
      ],
      "source": [
        "from cases.example_linear.model import FeedForward\n",
        "\n",
        "# 创建模型实例\n",
        "model = FeedForward()\n",
        "print(model)\n",
        "print(f\"\\n参数数量: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 数据缓存机制演示\n",
        "\n",
        "项目使用确定性的数据缓存来确保baseline和优化版本使用完全相同的输入：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用种子: 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fanjingluo/explore/llm4triton/harness/data.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  snapshot = torch.load(cache_file, map_location=\"cpu\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "输出形状: torch.Size([16, 1024])\n",
            "输出设备: cuda:0\n",
            "\n",
            "缓存目录: /home/fanjingluo/explore/llm4triton/artifacts/_shared_cache/example_linear\n",
            "缓存文件:\n",
            "  - inputs.pt: 65.62 KB\n",
            "  - weights.pt: 32790.37 KB\n"
          ]
        }
      ],
      "source": [
        "# 设置随机种子\n",
        "seed_used = seed_rng(42)\n",
        "print(f\"使用种子: {seed_used}\")\n",
        "\n",
        "# 创建forward函数（这会自动处理数据缓存）\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "forward = make_case_forward(\"example_linear\", device=device)\n",
        "\n",
        "# 执行一次forward，查看缓存位置\n",
        "output = forward()\n",
        "print(f\"\\n输出形状: {output.shape}\")\n",
        "print(f\"输出设备: {output.device}\")\n",
        "\n",
        "# 检查缓存文件\n",
        "cache_dir = ROOT / \"artifacts\" / \"_shared_cache\" / \"example_linear\"\n",
        "print(f\"\\n缓存目录: {cache_dir}\")\n",
        "if cache_dir.exists():\n",
        "    print(\"缓存文件:\")\n",
        "    for f in cache_dir.iterdir():\n",
        "        size_kb = f.stat().st_size / 1024\n",
        "        print(f\"  - {f.name}: {size_kb:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 运行Baseline性能测试\n",
        "\n",
        "使用评估框架运行baseline测试：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Baseline性能指标 ===\n",
            "延迟: 59.3219 ms\n",
            "吞吐量: 16.86 次/秒\n",
            "峰值显存: 106.25 MB\n",
            "正确性: True\n",
            "设备: cuda\n"
          ]
        }
      ],
      "source": [
        "# 运行baseline测试\n",
        "metrics_baseline = execute_variant(\n",
        "    label=\"baseline\",\n",
        "    forward_impl=forward,\n",
        "    warmup_iters=5,\n",
        "    measure_iters=20,\n",
        "    use_profiler=False\n",
        ")\n",
        "\n",
        "print(\"\\n=== Baseline性能指标 ===\")\n",
        "print(f\"延迟: {metrics_baseline['latency_ms']:.4f} ms\")\n",
        "print(f\"吞吐量: {metrics_baseline['throughput_per_s']:.2f} 次/秒\")\n",
        "if metrics_baseline['max_memory_bytes']:\n",
        "    memory_mb = metrics_baseline['max_memory_bytes'] / (1024 * 1024)\n",
        "    print(f\"峰值显存: {memory_mb:.2f} MB\")\n",
        "print(f\"正确性: {metrics_baseline['correctness']}\")\n",
        "print(f\"设备: {metrics_baseline['device_type']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 正确性验证演示\n",
        "\n",
        "运行一个\"candidate\"变体，框架会自动与baseline对比验证正确性：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Candidate性能指标 ===\n",
            "延迟: 58.5953 ms\n",
            "吞吐量: 17.07 次/秒\n",
            "正确性验证: ✓ 通过\n",
            "最大绝对误差: 0.00e+00\n",
            "最大相对误差: 0.00e+00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fanjingluo/explore/llm4triton/harness/runner.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  expected_snapshot = torch.load(reference_path, map_location=\"cpu\")\n"
          ]
        }
      ],
      "source": [
        "# 运行candidate测试（使用相同的forward实现作为演示）\n",
        "metrics_candidate = execute_variant(\n",
        "    label=\"candidate\",\n",
        "    forward_impl=forward,\n",
        "    warmup_iters=5,\n",
        "    measure_iters=20,\n",
        "    baseline_label=\"baseline\",\n",
        "    compare_outputs=True\n",
        ")\n",
        "\n",
        "print(\"\\n=== Candidate性能指标 ===\")\n",
        "print(f\"延迟: {metrics_candidate['latency_ms']:.4f} ms\")\n",
        "print(f\"吞吐量: {metrics_candidate['throughput_per_s']:.2f} 次/秒\")\n",
        "print(f\"正确性验证: {'✓ 通过' if metrics_candidate['correctness'] else '✗ 失败'}\")\n",
        "if 'correctness_max_abs_diff' in metrics_candidate:\n",
        "    print(f\"最大绝对误差: {metrics_candidate['correctness_max_abs_diff']:.2e}\")\n",
        "    print(f\"最大相对误差: {metrics_candidate['correctness_max_rel_diff']:.2e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 性能对比分析\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== 性能对比 ===\n",
            "Baseline延迟:   59.3219 ms\n",
            "Candidate延迟:  58.5953 ms\n",
            "加速比:         1.01x\n",
            "\n",
            "正确性:        True\n"
          ]
        }
      ],
      "source": [
        "# 性能对比\n",
        "speedup = metrics_baseline['latency_ms'] / metrics_candidate['latency_ms']\n",
        "\n",
        "print(\"\\n=== 性能对比 ===\")\n",
        "print(f\"Baseline延迟:   {metrics_baseline['latency_ms']:.4f} ms\")\n",
        "print(f\"Candidate延迟:  {metrics_candidate['latency_ms']:.4f} ms\")\n",
        "print(f\"加速比:         {speedup:.2f}x\")\n",
        "print(f\"\\n正确性:        {metrics_candidate['correctness']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 查看保存的指标文件\n",
        "\n",
        "所有指标都会自动保存到artifacts目录：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "保存的文件结构:\n",
            "\n",
            "baseline/\n",
            "  - metrics.json: 0.31 KB\n",
            "  - reference_outputs.pt: 65.22 KB\n",
            "\n",
            "candidate/\n",
            "  - metrics.json: 0.31 KB\n"
          ]
        }
      ],
      "source": [
        "# 查看保存的metrics\n",
        "baseline_metrics_path = ROOT / \"artifacts\" / \"baseline\" / \"metrics.json\"\n",
        "candidate_metrics_path = ROOT / \"artifacts\" / \"candidate\" / \"metrics.json\"\n",
        "\n",
        "print(\"保存的文件结构:\")\n",
        "artifacts_dir = ROOT / \"artifacts\"\n",
        "for variant_dir in [\"baseline\", \"candidate\"]:\n",
        "    var_path = artifacts_dir / variant_dir\n",
        "    if var_path.exists():\n",
        "        print(f\"\\n{variant_dir}/\")\n",
        "        for f in var_path.rglob('*'):\n",
        "            if f.is_file():\n",
        "                rel_path = f.relative_to(var_path)\n",
        "                size_kb = f.stat().st_size / 1024\n",
        "                print(f\"  - {rel_path}: {size_kb:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Profiler演示（可选）\n",
        "\n",
        "启用profiler来获取详细的性能分析：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "运行带profiler的测试...\n",
            "\n",
            "Profiler trace已保存到: /home/fanjingluo/explore/llm4triton/artifacts/profiled_baseline/profiler/trace.json\n",
            "文件大小: 0.13 MB\n",
            "\n",
            "你可以在Chrome浏览器中打开 chrome://tracing 来查看此文件\n"
          ]
        }
      ],
      "source": [
        "# 使用profiler运行（仅在CUDA可用时）\n",
        "if torch.cuda.is_available():\n",
        "    print(\"运行带profiler的测试...\")\n",
        "    metrics_profiled = execute_variant(\n",
        "        label=\"profiled_baseline\",\n",
        "        forward_impl=forward,\n",
        "        warmup_iters=2,\n",
        "        measure_iters=5,\n",
        "        use_profiler=True\n",
        "    )\n",
        "    \n",
        "    profiler_trace = ROOT / \"artifacts\" / \"profiled_baseline\" / \"profiler\" / \"trace.json\"\n",
        "    if profiler_trace.exists():\n",
        "        size_mb = profiler_trace.stat().st_size / (1024 * 1024)\n",
        "        print(f\"\\nProfiler trace已保存到: {profiler_trace}\")\n",
        "        print(f\"文件大小: {size_mb:.2f} MB\")\n",
        "        print(\"\\n你可以在Chrome浏览器中打开 chrome://tracing 来查看此文件\")\n",
        "else:\n",
        "    print(\"Profiler功能需要CUDA支持\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "这个项目提供了以下核心功能：\n",
        "\n",
        "1. **标准化评估框架**: 通过manifest.yaml定义测试案例，无需编写额外的测试代码\n",
        "2. **确定性数据缓存**: 确保baseline和优化版本使用完全相同的输入数据，保证公平对比\n",
        "3. **自动正确性验证**: 自动对比baseline和候选版本的输出，支持容差配置\n",
        "4. **性能分析**: 测量延迟、吞吐量、显存使用等关键指标\n",
        "5. **Profiler集成**: 支持PyTorch profiler，生成详细的性能trace\n",
        "6. **结果归档**: 所有指标和trace自动保存到artifacts目录\n",
        "\n",
        "下一步可以：\n",
        "- 创建自己的测试案例（参考cases/TEMPLATE/）\n",
        "- 实现Triton优化版本并对比性能\n",
        "- 使用LangGraph集成的agent进行自动化优化\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "profiler",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
